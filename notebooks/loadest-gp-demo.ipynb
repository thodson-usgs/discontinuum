{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4468d2cc-8303-40d7-8821-8b85c8c21e3e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# loadest-gp (prototype)\n",
    "LOAD ESTimator (LOADEST) is a software program for estimating some constituent using surrogate variables (covariates).\n",
    "However, LOADEST has several serious limitations, and it has been all but replaced by another model known as WRTDS.\n",
    "`loadest-gp` essentially reimplements WRTDS as a Gaussian process.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thodson-usgs/discontinuum/blob/main/notebooks/loadest-gp-demo.ipynb)\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/thodson-usgs/discontinuum/main?labpath=notebooks%2Floadest-gp-demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272692db-c31c-4f2d-9a66-bb904a85d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the latest version of discontinuum\n",
    "# !pip install git+https://github.com/thodson-usgs/discontinuum.git\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c139f8d5-ae66-4e7e-9c20-8a58a704a8dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102a05a4-2f32-4acc-ad3e-c3505b1e4846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "\n",
    "# SF Coeur D Alene River \n",
    "#site = \"12413470\"\n",
    "#start_date = \"1988-10-01\" \n",
    "#end_date = \"2021-09-30\" \n",
    "\n",
    "# Choptank River at Greensboro, MD\n",
    "site = \"01491000\" \n",
    "start_date = \"1979-10-01\"\n",
    "end_date = \"2011-09-30\"\n",
    "\n",
    "characteristic = 'Inorganic nitrogen (nitrate and nitrite)'\n",
    "fraction = 'Dissolved'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b013affa-c9ac-4015-b2a0-13b082be2ae8",
   "metadata": {},
   "source": [
    "First, download the data. In `discontinuum`, the convention is to download directly using `providers`, which wrap a data provider's web-service and perform some initial formatting and metadata construction, then return the result as an `xarray.Dataset`. Here, we'll uses the `usgs` provider. If you need data from another source, create a `provider` and ensure the output matches that of the `usgs` provider. Here, we'll download some daily streamflow data to use as our model's input, and some concentration samples as our target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4425fc1-a193-4f51-accc-5048d7805901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loadest_gp.providers import usgs\n",
    "\n",
    "# download covariates (daily streamflow)\n",
    "daily = usgs.get_daily(site=site, start_date=start_date, end_date=end_date)\n",
    "\n",
    "# download target (concentration)\n",
    "samples = usgs.get_samples(site=site, \n",
    "                           start_date=start_date, \n",
    "                           end_date=end_date, \n",
    "                           characteristic=characteristic, \n",
    "                           fraction=fraction)\n",
    "\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036dc34e-da51-4a1a-a2c0-b63a06a01f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.plot.scatter(x='time', y='concentration')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b8f59-58b0-47a6-86d0-d407c599e104",
   "metadata": {},
   "source": [
    "Next, perpare the training data by preforming an inner join of the target and covarites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89df570-5a21-4438-9091-8c10d69bfffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from discontinuum.utils import aggregate_to_daily\n",
    "\n",
    "samples = aggregate_to_daily(samples)\n",
    "\n",
    "training_data = xr.merge([samples, daily], join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf2188c-8e1b-401e-a7e2-b1d630621202",
   "metadata": {},
   "source": [
    "Now, we're ready to fit the model. Depending on your hardware, this can take seconds to several minutes. The first fit will also compiles the model, which takes longer. After running it once, try running the cell again and note the difference in wall time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0eef3-1f31-4950-897b-11713e17a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# select an engine\n",
    "# from loadest_gp import LoadestGPMarginalPyMC as LoadestGP\n",
    "from loadest_gp import LoadestGPMarginalGPyTorch as LoadestGP\n",
    "\n",
    "model = LoadestGP()\n",
    "\n",
    "model.fit(target=training_data['concentration'], covariates=training_data[['time','flow']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f04bfb6-b9d1-4c35-aa1d-4f073b7c494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot result\n",
    "model.plot(daily[['time','flow']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7557d315-60a0-404a-99c0-fef59f69c062",
   "metadata": {},
   "source": [
    "Like WRTDS, we can also plot the variable space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa71aa-82f5-4f09-8202-683b44bacba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.contourf(levels=5, y_scale='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c561ff69-bc37-493b-b414-18dfcbd07d9b",
   "metadata": {},
   "source": [
    "For plotting, we don't need to simulate the full covariance matrix. Instead, we can use its diagonal to compute confidence intervals, but for most other uses, we need to simulate predictions using full covariance, which is slower. Here, we simulate daily concentration during 1980-2010, then we will use those simulations to estimate annual fluxes with uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fa42a3-2414-445b-9c25-1c7d6af6cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate concentration\n",
    "sim_slice = daily[['time','flow']].sel(time=slice(\"1980\",\"2010\"))\n",
    "\n",
    "sim = model.sample(sim_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f7155-d3b3-452c-bb7c-0a80f60107d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first realization of concentration. \n",
    "sim.sel(draw=0).plot.line(x='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fb7ba8-c3d7-42a5-aa89-f228774b4608",
   "metadata": {},
   "source": [
    "In practice, we aren't interested in a single simulated timeseries. Rather, we'll take a large sample of these simulated series, then pass them through some function to simulate a probability distrubution. For example, if we were interested in some annual value. We'd pass the simulations through that annual function to estimate a probability distribution. We demonstrate this concent for estimating the annual nutrient flux.\n",
    "\n",
    "`loadest_gp` provides a couple convenience functions just for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf5a5b8-cedd-4d18-860f-9a0ea392a9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loadest_gp.utils import concentration_to_flux, plot_annual_flux\n",
    "\n",
    "flux = concentration_to_flux(sim, sim_slice['flow'])\n",
    "plot_annual_flux(flux)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a563850-1cab-4bfa-b393-e98e3b0aa3cb",
   "metadata": {},
   "source": [
    "In most streams, flow varies substantially from year-to-year, \n",
    "and we'd like to know what the flux might have been had flow been constant. \n",
    "In causal paralance, this is refered to as a [counterfactual](https://en.wikipedia.org/wiki/Counterfactual_conditional). \n",
    "However,`loadest_gp` isn't a causal model and can't provide us with counterfactuals. \n",
    "In other words, it can interpolate but not extrapolate. \n",
    "Nevertheless, we may treat is as such causal model and see what happens.\n",
    "Think of this type of analysis as a pseudo-counterfactual or educated guessing.\n",
    "There area variety of strategies that we might employee, \n",
    "some more sophisticated then others. \n",
    "At the end of the day, remember we're only guessing, \n",
    "so keep it simple and don't be tempted into over-interpretation.\n",
    "Here, we'll use a simple time substitution.\n",
    "Pick one year's worth of data and repeat it (except for the time variable) over the entire period of analysis.\n",
    "For example, let's repeat the year 1995 to fill in the data for our 1980-2010 in our counterfactual.\n",
    "What's special about 1995?\n",
    "Nothing, except that it is at the middle of our period.\n",
    "Choosing a different year should give similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57176f69-df5a-4b49-93d4-ec11d33aee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pseudo-counterfactual\n",
    "from discontinuum.utils import time_substitution\n",
    "\n",
    "counterfactual = time_substitution(sim_slice, interval=slice(\"1995\",\"1995\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e718984-c9a5-4c85-af25-4dbe088272c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate\n",
    "counterfactual_sim = model.sample(counterfactual)\n",
    "counterfactual_flux = concentration_to_flux(counterfactual_sim, sim_slice['flow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb2afdb-b4b4-4d6b-8429-59f3014fce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and plot the result\n",
    "plot_annual_flux(counterfactual_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6afc7e-116b-44b7-a0ed-617f219dd3c2",
   "metadata": {},
   "source": [
    "Now, the annual fluxes should be less variable than before, and the trend becomes apparent (depending on your choice of river)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
